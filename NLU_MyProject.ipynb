{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLU_MyProject.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN5QBH+fHWWT2Fnv89ESmaz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdoardoMaines/NLU-Project/blob/main/NLU_MyProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9Fwto6r44pkl"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import re\n",
        "import torch as d2l\n",
        "import nltk\n",
        "from nltk.corpus.reader import ConllCorpusReader\n",
        "from nltk.lm.vocabulary import Vocabulary\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#OPEN FILES\n",
        "def readFile(path):\n",
        "  corpus = []\n",
        "  with open(path) as file:\n",
        "    for line in file:\n",
        "      corpus.append(line)\n",
        "\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "zA8klMd8BoIP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SPLIT IN WORDS\n",
        "def tokenize(lines):\n",
        "  return [line.split() for line in lines]"
      ],
      "metadata": {
        "id": "ecz7tXwDET9Y"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CLASS VOCABOLARY\n",
        "class Vocab: \n",
        "  \"\"\"Vocabulary for text.\"\"\"\n",
        "  def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
        "    if tokens is None:\n",
        "      tokens = []\n",
        "    if reserved_tokens is None:\n",
        "      reserved_tokens = []\n",
        "\n",
        "  # Sort according to frequencies\n",
        "    counter = count_corpus(tokens)\n",
        "    self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
        "    reverse=True)\n",
        "    # The index for the unknown token is 0\n",
        "    self.idx_to_token = ['<unk>'] + reserved_tokens\n",
        "    self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
        "    for token, freq in self._token_freqs:\n",
        "      if freq < min_freq:\n",
        "        break\n",
        "      if token not in self.token_to_idx:\n",
        "        self.idx_to_token.append(token)\n",
        "        self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
        "  def __len__(self):\n",
        "    return len(self.idx_to_token)\n",
        "\n",
        "  def __getitem__(self, tokens):\n",
        "    if not isinstance(tokens, (list, tuple)):\n",
        "      return self.token_to_idx.get(tokens, self.unk)\n",
        "    return [self.__getitem__(token) for token in tokens]\n",
        "  \n",
        "  def to_tokens(self, indices):\n",
        "    if not isinstance(indices, (list, tuple)):\n",
        "      return self.idx_to_token[indices]\n",
        "    return [self.idx_to_token[index] for index in indices]\n",
        "\n",
        "\n",
        "  @property\n",
        "  def unk(self): # Index for the unknown token\n",
        "    return 0\n",
        "  @property\n",
        "  def token_freqs(self): # Index for the unknown token\n",
        "    return self._token_freqs\n",
        "\n",
        "\n",
        "def count_corpus(tokens):\n",
        "  \"\"\"Count token frequencies.\"\"\"\n",
        "  # Here `tokens` is a 1D list or 2D list\n",
        "  if len(tokens) == 0 or isinstance(tokens[0], list):\n",
        "  # Flatten a list of token lists into a list of tokens\n",
        "    tokens = [token for line in tokens for token in line]\n",
        "  return collections.Counter(tokens)"
      ],
      "metadata": {
        "id": "qcuaPLrpHYsU"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_corpus = readFile('/content/ptb.test.txt')\n",
        "train_corpus = readFile('/content/ptb.train.txt')\n",
        "valid_corpus = readFile('/content/ptb.valid.txt')\n",
        "\n",
        "#print(len(test_corpus))\n",
        "token = tokenize(test_corpus)\n",
        "for i in range(20):\n",
        "  print(token[i])"
      ],
      "metadata": {
        "id": "DVCOBkqB7XQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Vocab(token)\n",
        "print(list(vocab.token_to_idx.items())[:10]) ## in the dataset N = instead of numbers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zprnxcvIw72",
        "outputId": "9aefae92-c5bb-4d1d-9a7d-434b39c7604d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('<unk>', 0), ('the', 1), ('N', 2), ('of', 3), ('to', 4), ('a', 5), ('in', 6), ('and', 7), (\"'s\", 8), ('that', 9)]\n"
          ]
        }
      ]
    }
  ]
}