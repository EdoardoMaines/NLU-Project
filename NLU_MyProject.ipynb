{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLU_MyProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMpiKxwGgAqlF270LYVpGA3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdoardoMaines/NLU-Project/blob/main/NLU_MyProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Fwto6r44pkl"
      },
      "outputs": [],
      "source": [
        "!pip install d2l==0.17.0\n",
        "\n",
        "import collections\n",
        "import re\n",
        "from d2l import torch as d2l\n",
        "import nltk\n",
        "from nltk.corpus.reader import ConllCorpusReader\n",
        "from nltk.lm.vocabulary import Vocabulary\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#OPEN FILES\n",
        "def readFile(path):\n",
        "  corpus = []\n",
        "  with open(path) as file:\n",
        "    for line in file:\n",
        "      corpus.append(line)\n",
        "\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "zA8klMd8BoIP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SPLIT IN WORDS\n",
        "def tokenize(lines, token='word'):\n",
        "  if token == 'word':\n",
        "    return [line.split() for line in lines]\n",
        "  if token == 'char':\n",
        "    return [list(line) for line in lines]"
      ],
      "metadata": {
        "id": "ecz7tXwDET9Y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CLASS VOCABOLARY\n",
        "class Vocab: \n",
        "  \"\"\"Vocabulary for text.\"\"\"\n",
        "  def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
        "    if tokens is None:\n",
        "      tokens = []\n",
        "    if reserved_tokens is None:\n",
        "      reserved_tokens = []\n",
        "\n",
        "  # Sort according to frequencies\n",
        "    counter = count_corpus(tokens)\n",
        "    self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
        "    reverse=True)\n",
        "    # The index for the unknown token is 0\n",
        "    self.idx_to_token = ['<unk>'] + reserved_tokens\n",
        "    self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
        "    for token, freq in self._token_freqs:\n",
        "      if freq < min_freq:\n",
        "        break\n",
        "      if token not in self.token_to_idx:\n",
        "        self.idx_to_token.append(token)\n",
        "        self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
        "  def __len__(self):\n",
        "    return len(self.idx_to_token)\n",
        "\n",
        "  def __getitem__(self, tokens):\n",
        "    if not isinstance(tokens, (list, tuple)):\n",
        "      return self.token_to_idx.get(tokens, self.unk)\n",
        "    return [self.__getitem__(token) for token in tokens]\n",
        "  \n",
        "  def to_tokens(self, indices):\n",
        "    if not isinstance(indices, (list, tuple)):\n",
        "      return self.idx_to_token[indices]\n",
        "    return [self.idx_to_token[index] for index in indices]\n",
        "\n",
        "\n",
        "  @property\n",
        "  def unk(self): # Index for the unknown token\n",
        "    return 0\n",
        "  @property\n",
        "  def token_freqs(self): # Index for the unknown token\n",
        "    return self._token_freqs\n",
        "\n",
        "\n",
        "def count_corpus(tokens):\n",
        "  \"\"\"Count token frequencies.\"\"\"\n",
        "  # Here `tokens` is a 1D list or 2D list\n",
        "  if len(tokens) == 0 or isinstance(tokens[0], list):\n",
        "  # Flatten a list of token lists into a list of tokens\n",
        "    tokens = [token for line in tokens for token in line]\n",
        "  return collections.Counter(tokens)"
      ],
      "metadata": {
        "id": "qcuaPLrpHYsU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_corpus = readFile('/content/ptb.test.txt')\n",
        "train_corpus = readFile('/content/ptb.train.txt')\n",
        "valid_corpus = readFile('/content/ptb.valid.txt')\n",
        "\n",
        "#print(len(test_corpus))\n",
        "tokens = tokenize(test_corpus)\n",
        "\n",
        "# for i in range(20):\n",
        "#   print(tokens[i])"
      ],
      "metadata": {
        "id": "DVCOBkqB7XQh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Vocab(tokens)\n",
        "vocab2 = d2l.Vocab(tokens)\n",
        "print(list(vocab.token_to_idx.items())[:10]) ## in the dataset N = instead of numbers\n",
        "print(list(vocab2.token_to_idx.items())[:10])"
      ],
      "metadata": {
        "id": "_zprnxcvIw72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert each text line into a list of numerical indices\n",
        "\n",
        "for i in [0, 10]:\n",
        "  print('words:', tokens[i])\n",
        "  print('indices:', vocab[tokens[i]])\n",
        "\n"
      ],
      "metadata": {
        "id": "xOJO0oflcV1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#unigram\n",
        "tokens = d2l.tokenize(test_corpus)\n",
        "corpus = [token for line in tokens for token in line]\n",
        "vocab = d2l.Vocab(corpus)\n",
        "vocab.token_freqs[:10]"
      ],
      "metadata": {
        "id": "zx07wGJXplh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bigram\n",
        "bigram_token = [pair for pair in zip(corpus[:-1], corpus[1:])]\n",
        "bigram_vocab = d2l.Vocab(bigram_token)\n",
        "bigram_vocab.token_freqs[:10]"
      ],
      "metadata": {
        "id": "2BTTbfv9jQWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#trigram\n",
        "trigram_token = [pair for pair in zip(corpus[:-2], corpus[1:-1], corpus[2:])]\n",
        "trigram_vocab = d2l.Vocab(trigram_token)\n",
        "trigram_vocab.token_freqs[:10]"
      ],
      "metadata": {
        "id": "YV0F1nutrFKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "\n",
        "#get the data randomly\n",
        "def seq_data_iter_random(corpus, batch_size, num_steps):\n",
        "  \"\"\"Generate a minibatch of subsequences using random sampling.\"\"\"\n",
        "  # Start with a random offset (inclusive of `num_steps - 1`) to partition a\n",
        "  # sequence\n",
        "  corpus = corpus[random.randint(0, num_steps - 1):]\n",
        "  # Subtract 1 since we need to account for labels\n",
        "  num_subseqs = (len(corpus) - 1) // num_steps\n",
        "  # The starting indices for subsequences of length `num_steps`\n",
        "  initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
        "  # In random sampling, the subsequences from two adjacent random\n",
        "  # minibatches during iteration are not necessarily adjacent on the\n",
        "  # original sequence\n",
        "  random.shuffle(initial_indices)\n",
        "\n",
        "  def data(pos):\n",
        "  # Return a sequence of length `num_steps` starting from `pos`\n",
        "    return corpus[pos: pos + num_steps]\n",
        "\n",
        "\n",
        "\n",
        "  num_batches = num_subseqs // batch_size\n",
        "\n",
        "  for i in range(0, batch_size * num_batches, batch_size):\n",
        "    # Here, `initial_indices` contains randomized starting indices for\n",
        "    # subsequences\n",
        "    initial_indices_per_batch = initial_indices[i: i + batch_size]\n",
        "    X = [data(j) for j in initial_indices_per_batch]\n",
        "    Y = [data(j + 1) for j in initial_indices_per_batch]\n",
        "    yield torch.tensor(X), torch.tensor(Y)\n",
        "\n",
        "\n",
        "#get the data sequentially\n",
        "def seq_data_iter_sequential(corpus, batch_size, num_steps):\n",
        "  \"\"\"Generate a minibatch of subsequences using sequential partitioning.\"\"\"\n",
        "  # Start with a random offset to partition a sequence\n",
        "  offset = random.randint(0, num_steps)\n",
        "  num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
        "  Xs = torch.tensor(corpus[offset: offset + num_tokens])\n",
        "  Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n",
        "  Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
        "  num_batches = Xs.shape[1] // num_steps\n",
        "  for i in range(0, num_steps * num_batches, num_steps):\n",
        "    X = Xs[:, i: i + num_steps]\n",
        "    Y = Ys[:, i: i + num_steps]\n",
        "    yield X, Y"
      ],
      "metadata": {
        "id": "GA0pqAL3tfNr"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SeqDataLoader:\n",
        "  \"\"\"An iterator to load sequence data.\"\"\"\n",
        "  def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n",
        "    if use_random_iter:\n",
        "      self.data_iter_fn = seq_data_iter_random\n",
        "    else:\n",
        "      self.data_iter_fn = seq_data_iter_sequential\n",
        "      self.corpus = tokens[:max_tokens]\n",
        "      self.vocab = vocab[:max_tokens]\n",
        "      self.batch_size, self.num_steps = batch_size, num_steps\n",
        "\n",
        "  def __iter__(self):\n",
        "    return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)"
      ],
      "metadata": {
        "id": "BGTOYSpmu-c2"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(batch_size, num_steps, use_random_iter=False, max_tokens=10000):\n",
        "  \"\"\"Return the iterator and the vocabulary of the time machine dataset.\"\"\"\n",
        "  data_iter = SeqDataLoader(\n",
        "  batch_size, num_steps, use_random_iter, max_tokens)\n",
        "  return data_iter, data_iter.vocab"
      ],
      "metadata": {
        "id": "9r2MdR-svYb9"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CONCISE IMPLEMENTATION OF CNN\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from d2l import torch as d2l\n",
        "\n",
        "batch_size, num_steps = 32, 35\n",
        "train_iter, vocab = load_data(batch_size, num_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "b-Wmytt8xWX1",
        "outputId": "b96e260b-259f-4581-ff04-9d4cb178cfe2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-2090ffad0534>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m35\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-a45450be28dd>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(batch_size, num_steps, use_random_iter, max_tokens)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;34m\"\"\"Return the iterator and the vocabulary of the time machine dataset.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   data_iter = SeqDataLoader(\n\u001b[0;32m----> 4\u001b[0;31m   batch_size, num_steps, use_random_iter, max_tokens)\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-d7c03280fb4c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_size, num_steps, use_random_iter, max_tokens)\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_iter_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_data_iter_sequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/d2l/torch.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_to_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
          ]
        }
      ]
    }
  ]
}